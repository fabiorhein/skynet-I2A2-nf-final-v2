"""
Chat Agent for LLM-powered Q&A system.

This module provides intelligent chat capabilities that can:
- Answer questions about processed documents
- Analyze CSV data and provide insights
- Cache responses to save tokens
- Maintain conversation context
"""
import json
import hashlib
import asyncio
from typing import List, Dict, Any, Optional, Tuple, Union
from datetime import datetime, timedelta
from dataclasses import dataclass
import uuid
import logging
import re

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, SystemMessage

import os
import sys

# Adiciona o diret√≥rio raiz ao path para garantir que as importa√ß√µes funcionem
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

try:
    from config import GOOGLE_API_KEY, SUPABASE_URL, SUPABASE_KEY
except ImportError:
    # Tenta carregar do ambiente ou secrets do Streamlit
    import os
    import streamlit as st
    
    # Tenta obter do ambiente ou do secrets do Streamlit
    def get_config_value(key, default=None):
        return os.getenv(key) or getattr(st.secrets, key, default) if hasattr(st, 'secrets') else os.getenv(key, default)
    
    GOOGLE_API_KEY = get_config_value('GOOGLE_API_KEY')
    SUPABASE_URL = get_config_value('SUPABASE_URL') or get_config_value('connections.supabase.URL')
    SUPABASE_KEY = get_config_value('SUPABASE_KEY') or get_config_value('connections.supabase.KEY')

from backend.services.document_analyzer import DocumentAnalyzer

logger = logging.getLogger(__name__)


@dataclass
class ChatResponse:
    """Response from chat agent."""
    content: str
    metadata: Dict[str, Any]
    cached: bool = False
    tokens_used: Optional[int] = None


@dataclass
class DocumentContext:
    """Context information from documents."""
    documents: List[Dict[str, Any]]
    summaries: List[str]
    insights: List[Dict[str, Any]]


class DocumentSearchEngine:
    """Search and retrieve relevant document context."""

    def __init__(self, storage):
        self.storage = storage

    async def search_documents(
        self,
        query: str,
        limit: int = 5,
        document_types: Optional[List[str]] = None
    ) -> DocumentContext:
        """Search for relevant documents based on query."""

        try:
            # Use storage to get all documents and filter by query
            all_docs = await self.storage.get_fiscal_documents(page=1, page_size=1000)

            # Filter documents containing the query
            filtered_docs = []
            for doc in all_docs.items:
                if self._document_matches_query(doc, query):
                    filtered_docs.append(doc)

            # Limit results
            documents = filtered_docs[:limit]

            # Get summaries for these documents
            summaries = []
            insights = []

            for doc in documents:
                # Get document summaries if available
                doc_summaries = await self.storage.get_fiscal_documents(
                    id=doc['id'], page=1, page_size=1
                )
                if doc_summaries.items:
                    # Extract summary text from document data (if stored)
                    pass

                # Get analysis insights for this document
                doc_insights = await self.storage.get_fiscal_documents(
                    id=doc['id'], page=1, page_size=1
                )
                if doc_insights.items and doc_insights.items[0].get('classification'):
                    # Extract insights from classification data
                    pass

            return DocumentContext(
                documents=documents,
                summaries=summaries,
                insights=insights
            )

        except Exception as e:
            logger.error(f"Error searching documents: {e}")
            return DocumentContext(documents=[], summaries=[], insights=[])

    def _document_matches_query(self, document: Dict[str, Any], query: str) -> bool:
        """Check if document matches the search query."""
        query_lower = query.lower()
        extracted_data = document.get('extracted_data', {})

        # Search in various fields
        search_fields = [
            extracted_data.get('emitente', {}).get('razao_social', ''),
            extracted_data.get('emitente', {}).get('cnpj', ''),
            extracted_data.get('destinatario', {}).get('razao_social', ''),
            extracted_data.get('destinatario', {}).get('cnpj', ''),
            document.get('file_name', ''),
            document.get('document_number', ''),
            str(extracted_data)
        ]

        return any(query_lower in str(field).lower() for field in search_fields)


class AnalysisCache:
    """Cache system to avoid redundant LLM calls."""

    def __init__(self, storage):
        self.storage = storage

    def _generate_cache_key(self, query: str, context: Dict[str, Any]) -> str:
        """Generate a unique cache key for query + context."""
        context_str = json.dumps(context, sort_keys=True)
        combined = f"{query}|{context_str}"
        return hashlib.sha256(combined.encode()).hexdigest()

    async def get_cached_response(
        self,
        query: str,
        context: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """Get cached response if available and not expired."""

        cache_key = self._generate_cache_key(query, context)

        try:
            cache_entry = self.storage.get_analysis_cache(cache_key)

            if cache_entry:
                return {
                    'content': cache_entry.get('response_content', ''),
                    'metadata': cache_entry.get('response_metadata', {}),
                    'cached': True
                }

        except Exception as e:
            logger.error(f"Error retrieving cache: {e}")

        return None

    async def cache_response(
        self,
        query: str,
        context: Dict[str, Any],
        response: str,
        metadata: Dict[str, Any],
        query_type: str = 'general'
    ) -> None:
        """Cache the response for future use."""

        cache_key = self._generate_cache_key(query, context)
        expires_at = (datetime.now() + timedelta(days=7)).isoformat()

        try:
            self.storage.save_analysis_cache(
                cache_key=cache_key,
                query_type=query_type,
                query_text=query,
                context_data=context,
                response_content=response,
                response_metadata=metadata,
                expires_at=expires_at
            )

        except Exception as e:
            logger.error(f"Error caching response: {e}")


class ChatAgent:
    """Main chat agent that coordinates LLM interactions."""

    def __init__(self, storage):
        self.storage = storage
        self.search_engine = DocumentSearchEngine(storage)
        self.cache = AnalysisCache(storage)
        self.document_analyzer = DocumentAnalyzer(storage)

        # Initialize Gemini
        if not GOOGLE_API_KEY:
            logger.error("GOOGLE_API_KEY not configured")
            self.model = None
        else:
            # Configura√ß√£o do modelo Gemini - tentar 2.0-flash primeiro, depois 1.5-flash
            try:
                # Tentar modelo mais avan√ßado primeiro
                model_name = 'gemini-2.0-flash-exp'
                try:
                    self.model = ChatGoogleGenerativeAI(
                        model=model_name,
                        google_api_key=GOOGLE_API_KEY,
                        temperature=0.0,
                        request_timeout=30
                    )
                    self.model_name = model_name
                    logger.info(f"‚úÖ Successfully initialized Gemini model: {model_name}")
                except Exception as e:
                    logger.warning(f"Gemini 2.0-flash not available: {e}")
                    # Fallback para 1.5-flash
                    model_name = 'gemini-1.5-flash'
                    self.model = ChatGoogleGenerativeAI(
                        model=model_name,
                        google_api_key=GOOGLE_API_KEY,
                        temperature=0.0,
                        request_timeout=30
                    )
                    self.model_name = model_name
                    logger.info(f"‚úÖ Successfully initialized fallback Gemini model: {model_name}")
            except Exception as e:
                error_msg = str(e)
                logger.error(f"‚ùå Failed to initialize Gemini models: {error_msg}")
                if "quota" in error_msg.lower() or "429" in error_msg:
                    friendly_msg = (
                        "Limite de quota da API do Gemini excedido.\n\n"
                        "üîÑ **Verifique seu plano e tente novamente:**\n"
                        "1. Acesse: https://ai.google.dev/gemini-api/docs/rate-limits\n"
                        "2. Considere usar uma chave API diferente\n"
                        "3. Aguarde a libera√ß√£o da quota (geralmente 1-2 horas)\n\n"
                        "üí° Alternativamente, configure uma chave API do OpenAI em .streamlit/secrets.toml"
                    )
                else:
                    friendly_msg = (
                        "Erro ao inicializar modelos Gemini.\n\n"
                        "üí° Para resolver:\n"
                        "1. Verifique se a GOOGLE_API_KEY est√° correta em .streamlit/secrets.toml\n"
                        "2. Certifique-se de que sua conta tem acesso aos modelos Gemini\n"
                        "3. Considere usar uma chave API do OpenAI como alternativa"
                    )
                raise Exception(friendly_msg) from e

        # Simple conversation history (replaces deprecated LangChain memory)
        self.conversation_history = {}

        # System prompt
        self.system_prompt = """
        Voc√™ √© um assistente especialista em documentos fiscais brasileiros. Responda sempre em portugu√™s de forma clara, precisa e √∫til.

        **Diretrizes Gerais:**
        1. Use os dados fornecidos pelo sistema para responder com 100% de precis√£o
        2. Seja espec√≠fico com n√∫meros, quantidades e valores
        3. Use formata√ß√£o markdown para melhorar a legibilidade
        4. Responda de forma natural e conversacional
        5. Foque nos dados reais, n√£o em conhecimento geral

        **Para perguntas sobre contagem:**
        - Destaque o total de documentos
        - Mencione o valor total se dispon√≠vel
        - Inclua distribui√ß√£o por categoria com percentuais
        - Seja direto e informativo

        **Para pedidos de lista:**
        - Apresente os documentos de forma organizada
        - Inclua tipo, emissor, valor e data quando dispon√≠vel
        - Use tabelas ou listas numeradas para clareza
        - Mencione se est√° mostrando apenas parte dos documentos

        **Para resumos/categorias:**
        - Foque na distribui√ß√£o por tipo de documento
        - Inclua percentuais para cada categoria
        - Mencione os principais emissores
        - Destaque padr√µes ou observa√ß√µes relevantes

        **Para perguntas espec√≠ficas:**
        - Use o contexto fornecido para responder
        - Seja preciso com os dados dispon√≠veis
        - Explique se alguma informa√ß√£o n√£o est√° dispon√≠vel

        Responda sempre baseado nos dados fornecidos, n√£o invente informa√ß√µes.
        """

    async def create_session(self, session_name: str = None) -> str:
        """Create a new chat session."""

        try:
            session = self.storage.create_chat_session(session_name)
            return session['id']

        except Exception as e:
            logger.error(f"Error creating chat session: {e}")
            raise

    async def save_message(
        self,
        session_id: str,
        message_type: str,
        content: str,
        metadata: Dict[str, Any] = None
    ) -> None:
        """Save a message to the chat session."""

        try:
            self.storage.save_chat_message(session_id, message_type, content, metadata or {})

        except Exception as e:
            logger.error(f"Error saving message: {e}")

    async def get_conversation_history(self, session_id: str) -> List[Dict[str, Any]]:
        """Get conversation history for display."""
        try:
            messages = self.storage.get_chat_messages(session_id, limit=50)
            # Convert to the format expected by the frontend
            return messages

        except Exception as e:
            logger.error(f"Error getting conversation history: {e}")
            return []

    async def get_conversation_context(self, session_id: str) -> str:
        """Get conversation history as context for the LLM."""
        try:
            return self.storage.get_chat_context(session_id, limit=5)

        except Exception as e:
            logger.error(f"Error getting conversation context: {e}")
            return "Erro ao carregar hist√≥rico da conversa."

    async def _get_document_summary(self, filters: Optional[Dict] = None) -> Dict[str, Any]:
        """Obt√©m um resumo dos documentos com base nos filtros fornecidos."""
        return await self.document_analyzer.get_documents_summary(filters)

    async def _get_all_documents_summary(self, time_filter=None) -> Dict[str, Any]:
        """
        Obt√©m um resumo dos documentos para an√°lise de categorias.
        
        Args:
            time_filter: Filtra documentos criados ap√≥s esta data/hora.
                        Se None, retorna todos os documentos.
        """
        return await self.document_analyzer.get_all_documents_summary(time_filter=time_filter)

    def _is_summary_request(self, query: str) -> bool:
        """Verifica se a pergunta √© sobre resumo/categorias de documentos."""
        query_lower = query.lower()
        summary_keywords = [
            'resumo', 'categorias', 'tipos de documentos', 'distribui√ß√£o',
            'quantos documentos', 'quais categorias', 'quais tipos',
            'vis√£o geral', 'an√°lise geral', 'estat√≠sticas', 'estatistica',
            'estat√≠sticas dos documentos', 'estatistica dos documentos',
            'resumo dos documentos', 'categorias de documentos', 'tipos de nota',
            'quais s√£o as categorias', 'quantos de cada tipo', 'distribui√ß√£o de documentos'
        ]
        count_keywords = [
            'quantidade total', 'quantos documentos', 'quantas notas',
            'total de notas', 'n√∫mero total', 'contagem total'
        ]
        list_keywords = ['lista', 'listar', 'todos os documentos', 'todas as notas', 'me traga uma lista']

        has_summary = any(keyword in query_lower for keyword in summary_keywords)
        has_count = any(keyword in query_lower for keyword in count_keywords)
        has_list = any(keyword in query_lower for keyword in list_keywords)

        return has_summary and not has_count and not has_list

    def _is_list_request(self, query: str) -> bool:
        """Verifica se a consulta √© um pedido de listagem de documentos."""
        query_lower = query.lower()
        
        # Lista de palavras-chave que indicam um pedido de listagem
        list_keywords = [
            'listar', 'mostrar', 'quais s√£o', 'quais foram', 'quais s√£o as',
            'mostre', 'mostrar', 'todos os', 'todas as', '√∫ltimas', 'recentes',
            'notas fiscais', 'documentos fiscais', 'notas', 'documentos',
            'lista de', 'listagem de', 'relat√≥rio de', 'relatorio de',
            '√∫ltimos', '√∫ltimas', '√∫ltimo', '√∫ltima', 'importados', 'importadas',
            'minutos', 'hora', 'horas', 'dia', 'dias', 'semana', 'semanas'
        ]
        
        # Verifica se a consulta cont√©m pelo menos uma palavra-chave de listagem
        return any(keyword in query_lower for keyword in list_keywords)

    def _is_count_request(self, query: str) -> bool:
        """Verifica se a pergunta √© sobre contagem espec√≠fica."""
        query_lower = query.lower()
        count_keywords = [
            'quantidade total', 'quantos documentos', 'quantas notas',
            'total de notas', 'n√∫mero total', 'contagem total'
        ]

        return any(keyword in query_lower for keyword in count_keywords)

    def _prepare_summary_prompt(self, query: str, summary_data: Dict[str, Any]) -> str:
        """Prepara o contexto para perguntas de resumo de documentos."""
        context_parts = []

        # Informa√ß√µes b√°sicas
        total_docs = summary_data['total_documents']
        context_parts.append(f"üìä **Informa√ß√µes do Banco de Dados:**")
        context_parts.append(f"- Total de documentos: **{total_docs}**")
        context_parts.append(f"- Valor total dos documentos: **R$ {summary_data['total_value']:,.2f}**")
        context_parts.append("")

        # Categorias por tipo
        if summary_data['by_type']:
            context_parts.append("üìã **Distribui√ß√£o por Categoria:**")
            for category, count in summary_data['by_type'].items():
                percentage = (count / total_docs) * 100 if total_docs > 0 else 0
                context_parts.append(f"- **{category}**: {count} documentos ({percentage:.1f}%)")

        # Emissores principais
        if summary_data['by_issuer']:
            context_parts.append("")
            context_parts.append("üè¢ **Principais Emissores:**")
            sorted_issuers = sorted(summary_data['by_issuer'].items(), key=lambda x: x[1], reverse=True)
            for issuer, count in sorted_issuers[:5]:  # Top 5 emissores
                context_parts.append(f"- **{issuer}**: {count} documentos")

        # Instru√ß√µes para resposta
        context_parts.append("")
        context_parts.append("üìù **Instru√ß√µes para a resposta:**")
        context_parts.append("1. Use os dados fornecidos para criar um resumo preciso")
        context_parts.append("2. Apresente as informa√ß√µes em formato de tabela quando apropriado")
        context_parts.append("3. Seja espec√≠fico sobre quantidades e categorias")
        context_parts.append("4. Destaque informa√ß√µes importantes em negrito")
        context_parts.append("5. Responda sempre em portugu√™s claro e objetivo")

        return "\n".join(context_parts)

    async def _search_documents(self, query: str, limit: int = 5) -> List[Dict]:
        """Busca documentos relevantes usando busca sem√¢ntica."""
        return await self.document_analyzer.search_documents(query, limit)

    async def generate_response(
        self,
        session_id: str,
        query: str,
        context: Optional[Dict[str, Any]] = None
    ) -> ChatResponse:
        """Generate a response using LLM with caching and context."""

        # Check if Gemini is available
        if not self.model:
            error_message = "API do Google Gemini n√£o configurada. Verifique se a GOOGLE_API_KEY est√° definida em .streamlit/secrets.toml"
            await self.save_message(session_id, 'assistant', error_message, {'error': True})
            return ChatResponse(
                content=error_message,
                metadata={'error': True, 'error_type': 'missing_api_key'},
                cached=False
            )

        # Check cache first
        if context:
            cached = await self.cache.get_cached_response(query, context)
            if cached:
                await self.save_message(session_id, 'assistant', cached['content'],
                                      {'cached': True, 'tokens_used': 0})
                return ChatResponse(
                    content=cached['content'],
                    metadata=cached['metadata'],
                    cached=True
                )

        # Get relevant document context
        document_context = DocumentContext(documents=[], summaries=[], insights=[])

        try:
            # Determinar o tipo de pergunta e responder adequadamente
            if self._is_count_request(query):
                # Para perguntas sobre contagem total
                return await self._handle_count_request(session_id, query)

            elif self._is_list_request(query):
                # Para perguntas sobre lista de documentos
                return await self._handle_list_request(session_id, query)

            elif self._is_summary_request(query):
                # Para perguntas sobre resumo/categorias
                return await self._handle_summary_request(session_id, query)

            else:
                # Para perguntas espec√≠ficas ou gerais - usar busca normal
                return await self._handle_specific_search(session_id, query, context)

        except Exception as e:
            logger.error(f"Erro ao processar pergunta: {e}")
            # Fallback para busca gen√©rica
            return await self._handle_specific_search(session_id, query, context)

    async def _handle_count_request(self, session_id: str, query: str) -> ChatResponse:
        """Handle requests for document counts using LLM for natural response."""
        try:
            summary_data = await self._get_all_documents_summary()

            if not summary_data or summary_data['total_documents'] == 0:
                # Use Gemini for natural response even when no documents
                prompt = f"""O usu√°rio perguntou sobre a quantidade de documentos no banco de dados.

**Dados encontrados:**
- Total de documentos: 0
- N√£o h√° documentos para an√°lise

Por favor, responda de forma natural e informativa sobre a aus√™ncia de documentos no sistema."""
            else:
                # Prepare raw data for Gemini
                total = summary_data['total_documents']
                total_value = summary_data['total_value']
                categories = summary_data['by_type']
                issuers = summary_data['by_issuer']

                prompt = f"""O usu√°rio perguntou sobre a quantidade total de documentos no banco de dados.

**Dados brutos do banco:**
- Total de documentos fiscais: {total}
- Valor total dos documentos: R$ {total_value:,.2f}
- N√∫mero de categorias diferentes: {len(categories)}
- N√∫mero de emissores diferentes: {len(issuers)}

**Categorias encontradas:**
"""
                for category, count in categories.items():
                    percentage = (count / total) * 100
                    prompt += f"- {category}: {count} documentos ({percentage:.1f}%)\n"

                prompt += f"""
**Principais emissores:**
"""
                sorted_issuers = sorted(issuers.items(), key=lambda x: x[1], reverse=True)
                for issuer, count in sorted_issuers[:5]:
                    prompt += f"- {issuer}: {count} documentos\n"

                prompt += f"""

**Instru√ß√µes:**
Responda de forma natural e conversacional, como se estivesse falando diretamente com o usu√°rio.
Use os dados fornecidos para dar uma resposta precisa e √∫til.
Estruture a resposta de forma clara, usando negrito para destacar n√∫meros importantes.
Seja espec√≠fico sobre quantidades e categorias encontradas.
Responda em portugu√™s."""

            # Send to Gemini for natural formatting
            messages = [
                SystemMessage(content=self.system_prompt),
                HumanMessage(content=prompt)
            ]

            response = self.model.invoke(messages, config={
                'temperature': 0.1,
                'max_tokens': 800,
                'top_p': 0.9,
                'frequency_penalty': 0.3,
                'presence_penalty': 0.3
            })

            content = response.content if hasattr(response, 'content') else str(response)
            content = self._clean_response_content(content)

            # Create metadata
            metadata = {
                'model': self.model_name or 'unknown',
                'timestamp': datetime.now().isoformat(),
                'tokens_used': len(content.split()),
                'query_type': 'count',
                'raw_data': summary_data
            }

            await self.save_message(session_id, 'assistant', content, metadata)

            return ChatResponse(
                content=content,
                metadata=metadata,
                cached=False,
                tokens_used=metadata['tokens_used']
            )

        except Exception as e:
            error_message = f"Erro ao buscar contagem de documentos: {str(e)}"
            await self.save_message(session_id, 'assistant', error_message, {'error': True})
            return ChatResponse(content=error_message, metadata={'error': True}, cached=False)

    def _get_metadata_template(self, is_recent_query: bool = False, error: bool = False) -> Dict[str, Any]:
        """Return a standardized metadata template.
        
        Args:
            is_recent_query: Whether this is a recent documents query
            error: Whether this is an error response
            
        Returns:
            Dict with standardized metadata structure
        """
        return {
            'model': 'system' if error else (self.model_name or 'unknown'),
            'timestamp': datetime.now().isoformat(),
            'tokens_used': 0,
            'query_type': 'error' if error else ('recent_documents' if is_recent_query else 'list'),
            'document_count': 0,
            'total_documents': 0,
            'is_recent_query': is_recent_query,
            **({'error': True} if error else {})
        }

    async def _handle_list_request(self, session_id: str, query: str) -> ChatResponse:
        """Handle requests for document lists using LLM for natural response."""
        try:
            query_lower = query.lower()
            time_filter = None
            
            # Verifica se √© um pedido de √∫ltimas notas/dados recentes
            is_recent_query = any(word in query_lower for word in [
                '√∫ltimas notas', 'notas recentes', '√∫ltimos documentos', 
                'documentos recentes', 'notas fiscais recentes', '√∫ltimas notas fiscais',
                'mais recentes', 'notas mais recentes', 'documentos mais recentes',
                '√∫ltimos lan√ßamentos', '10 √∫ltimas notas', 'dez √∫ltimas notas',
                '√∫ltimos registros', 'registros recentes'
            ])
            
            # Verifica se h√° um filtro de tempo espec√≠fico na consulta
            import re
            from datetime import datetime, timedelta
            
            time_patterns = [
                (r'(\d+)\s*minutos?\s*atr√°s', 'minutes'),
                (r'(\d+)\s*horas?\s*atr√°s', 'hours'),
                (r'(\d+)\s*dias?\s*atr√°s', 'days'),
                (r'(\d+)\s*semanas?\s*atr√°s', 'weeks'),
                (r'√∫ltimos?\s*(\d+)\s*minutos?', 'minutes'),
                (r'√∫ltimas?\s*(\d+)\s*horas?', 'hours'),
                (r'√∫ltimos?\s*(\d+)\s*dias?', 'days'),
                (r'√∫ltimas?\s*(\d+)\s*semanas?', 'weeks'),
                (r'nos\s*√∫ltimos?\s*(\d+)\s*minutos?', 'minutes'),
                (r'nas\s*√∫ltimas?\s*(\d+)\s*horas?', 'hours'),
                (r'nos\s*√∫ltimos?\s*(\d+)\s*dias?', 'days'),
                (r'nas\s*√∫ltimas?\s*(\d+)\s*semanas?', 'weeks'),
            ]
            
            for pattern, unit in time_patterns:
                match = re.search(pattern, query_lower)
                if match:
                    value = int(match.group(1))
                    delta = timedelta(**{unit: value})
                    time_filter = datetime.now() - delta
                    is_recent_query = True
                    break
            
            # Busca os documentos com ordena√ß√£o por data e filtro de tempo
            summary_data = await self._get_all_documents_summary(time_filter=time_filter)
            documents_to_show = []  # Initialize as empty list
            total = 0
            
            if not summary_data or summary_data['total_documents'] == 0:
                # Return early with a friendly message when no documents are found
                message = "üì≠ N√£o foram encontrados documentos no sistema com os crit√©rios fornecidos."
                metadata = self._get_metadata_template(is_recent_query=is_recent_query)
                await self.save_message(session_id, 'assistant', message, metadata)
                return ChatResponse(content=message, metadata=metadata, cached=False)
            else:
                # Prepara os dados brutos para o Gemini
                total = summary_data['total_documents']
                documents = summary_data['documents']
                
                # Ordena os documentos por data de cria√ß√£o (mais recentes primeiro)
                documents_sorted = sorted(
                    documents, 
                    key=lambda x: x.get('created_at', ''), 
                    reverse=True
                )
                
                # Para consultas de '√∫ltimas notas', limita a 10 itens
                limit = 10 if is_recent_query else 15
                documents_to_show = documents_sorted[:limit]
                
                # Prepara o prompt baseado no tipo de consulta
                if is_recent_query:
                    prompt = """O usu√°rio pediu uma lista com as notas fiscais mais recentes do banco de dados.

**Dados brutos encontrados:**
- Total de documentos: {}
- Valor total: R$ {:.2f}
- Mostrando as {} notas mais recentes:

""".format(total, summary_data['total_value'], len(documents_to_show))
                else:
                    prompt = """O usu√°rio pediu uma lista com todos os documentos fiscais do banco de dados.

**Dados brutos encontrados:**
- Total de documentos: {}
- Valor total: R$ {:.2f}
- Mostrando {} de {} documentos:

""".format(total, summary_data['total_value'], len(documents_to_show), total)
                
                # Adiciona detalhes de cada documento
                for i, doc in enumerate(documents_to_show, 1):
                    doc_type = doc.get('categorized_type', 'N/A')
                    file_name = doc.get('file_name', 'N/A')
                    cnpj = doc.get('issuer_cnpj', 'N/A')
                    
                    # Formata a data e hora de forma mais amig√°vel
                    created_at = doc.get('created_at')
                    if created_at:
                        try:
                            dt = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                            # Formata como "DD/MM/YYYY √†s HH:MM"
                            formatted_date = dt.strftime('%d/%m/%Y √†s %H:%M')
                        except (ValueError, AttributeError):
                            formatted_date = created_at[:10]  # Pega apenas a data se n√£o conseguir converter
                    else:
                        formatted_date = 'Data n√£o dispon√≠vel'
                    
                    # Extrai o valor
                    value = 'N/A'
                    if doc.get('extracted_data'):
                        try:
                            data = doc['extracted_data']
                            if isinstance(data, str):
                                data = json.loads(data)
                            if isinstance(data, dict):
                                value = data.get('total', data.get('valor_total', data.get('value', 'N/A')))
                                # Formata o valor monet√°rio
                                if value not in ['N/A', None]:
                                    try:
                                        value = float(value)
                                        value = f'R$ {value:,.2f}'.replace('.', 'v').replace(',', '.').replace('v', ',')
                                    except (ValueError, TypeError):
                                        pass
                        except Exception as e:
                            logger.warning(f"Erro ao extrair valor do documento: {e}")
                    
                    # Adiciona detalhes do documento ao prompt
                    prompt += f"{i}. **{doc_type}**\n"
                    prompt += f"   üìÑ **Arquivo:** {file_name}\n"
                    prompt += f"   üè¢ **CNPJ Emissor:** {cnpj}\n"
                    prompt += f"   üí∞ **Valor:** {value}\n"
                    prompt += f"   üìÖ **Data/Hora:** {formatted_date}\n"
                    # Adiciona status de valida√ß√£o se dispon√≠vel
                    if doc.get('validation_status'):
                        status_emoji = '‚úÖ' if doc['validation_status'] == 'valid' else '‚ö†Ô∏è' if doc['validation_status'] == 'warning' else '‚ùå'
                        prompt += f"   {status_emoji} **Status:** {doc['validation_status'].capitalize()}\n"
                    prompt += "\n"

                if not is_recent_query and total > limit:
                    prompt += f"*(Mostrando apenas os primeiros {limit} documentos. Total no banco: {total})*\n\n"
                elif is_recent_query and len(documents_to_show) < total:
                    prompt += f"*(Mostrando as {len(documents_to_show)} notas mais recentes. Total no banco: {total})*\n\n"

                # Instru√ß√µes para a IA
                prompt += """**Instru√ß√µes:**
Responda de forma natural e conversacional, como se estivesse apresentando os documentos para o usu√°rio.
- Se for uma consulta por notas recentes, destaque que s√£o as mais atuais
- Inclua informa√ß√µes importantes como tipo, emissor, valor e data/hora
- Formate os valores monet√°rios corretamente (R$ X.XXX,XX)
- Use formata√ß√£o markdown para melhorar a legibilidade (negrito, it√°lico, listas)
- Seja espec√≠fico sobre quantos documentos est√£o sendo mostrados
- Inclua o total de documentos no banco para refer√™ncia
- Responda em portugu√™s"""

            try:
                # Envia para o Gemini para formata√ß√£o natural
                messages = [
                    SystemMessage(content=self.system_prompt),
                    HumanMessage(content=prompt)
                ]

                response = self.model.invoke(messages, config={
                    'temperature': 0.2,
                    'max_tokens': 2000,  # Aumentado para permitir respostas mais completas
                    'top_p': 0.9,
                    'frequency_penalty': 0.3,
                    'presence_penalty': 0.3
                })

                content = response.content if hasattr(response, 'content') else str(response)
                content = self._clean_response_content(content)

                # Create tracking metadata using the template
                metadata = self._get_metadata_template(is_recent_query=is_recent_query)
                metadata.update({
                    'model': self.model_name or 'unknown',
                    'tokens_used': len(content.split()),
                    'document_count': len(documents_to_show) if documents_to_show else 0,
                    'total_documents': total
                })
            except Exception as e:
                logger.error(f"Erro ao processar resposta do modelo: {str(e)}")
                content = "üîç Desculpe, ocorreu um erro ao processar sua solicita√ß√£o. Tente novamente mais tarde."
                metadata = self._get_metadata_template(is_recent_query=is_recent_query, error=True)
                metadata['error'] = str(e)

            await self.save_message(session_id, 'assistant', content, metadata)

            return ChatResponse(
                content=content,
                metadata=metadata,
                cached=False,
                tokens_used=metadata['tokens_used']
            )

        except Exception as e:
            error_message = f"Erro ao buscar lista de documentos: {str(e)}"
            logger.error(f"Error in _handle_list_request: {str(e)}", exc_info=True)
            metadata = self._get_metadata_template(error=True)
            metadata['error'] = str(e)
            await self.save_message(session_id, 'assistant', error_message, metadata)
            return ChatResponse(content=error_message, metadata=metadata, cached=False)

    async def _handle_summary_request(self, session_id: str, query: str) -> ChatResponse:
        """Handle requests for document summaries using LLM for natural response."""
        try:
            summary_data = await self._get_all_documents_summary()

            if not summary_data or summary_data['total_documents'] == 0:
                # Use Gemini for natural response even when no documents
                prompt = f"""O usu√°rio pediu um resumo das categorias dos documentos fiscais, mas n√£o foram encontrados documentos no banco de dados.

Por favor, responda de forma natural explicando que n√£o h√° documentos dispon√≠veis para an√°lise."""
            else:
                # Prepare raw data for Gemini
                total = summary_data['total_documents']
                total_value = summary_data['total_value']
                categories = summary_data['by_type']
                issuers = summary_data['by_issuer']

                prompt = f"""O usu√°rio pediu um resumo das categorias dos documentos fiscais no sistema.

**Dados brutos do banco de dados:**
- Total de documentos fiscais: {total}
- Valor total dos documentos: R$ {total_value:,.2f}
- N√∫mero de categorias diferentes: {len(categories)}
- N√∫mero de emissores diferentes: {len(issuers)}

**Distribui√ß√£o por categoria (dados brutos):**
"""
                for category, count in categories.items():
                    percentage = (count / total) * 100
                    prompt += f"- {category}: {count} documentos ({percentage:.1f}%)\n"

                prompt += f"""

**Principais emissores (dados brutos):**
"""
                sorted_issuers = sorted(issuers.items(), key=lambda x: x[1], reverse=True)
                for issuer, count in sorted_issuers[:5]:
                    prompt += f"- {issuer}: {count} documentos\n"

                prompt += f"""

**Instru√ß√µes para resposta:**
Responda de forma natural e conversacional, como se estivesse analisando os dados e explicando para o usu√°rio.
Use os dados fornecidos para criar um resumo claro e informativo.
Estruture a resposta de forma organizada, destacando:
1. O total geral de documentos
2. A distribui√ß√£o por categoria com percentuais
3. Os principais emissores
4. Observa√ß√µes relevantes sobre os dados

Use formata√ß√£o markdown (negrito, tabelas, listas) para melhorar a legibilidade.
Seja espec√≠fico e use os n√∫meros exatos do banco de dados.
Responda em portugu√™s de forma profissional e √∫til."""

            # Send to Gemini for natural formatting
            messages = [
                SystemMessage(content=self.system_prompt),
                HumanMessage(content=prompt)
            ]

            response = self.model.invoke(messages, config={
                'temperature': 0.2,
                'max_tokens': 1000,
                'top_p': 0.9,
                'frequency_penalty': 0.3,
                'presence_penalty': 0.3
            })

            content = response.content if hasattr(response, 'content') else str(response)
            content = self._clean_response_content(content)

            # Create metadata
            metadata = {
                'model': self.model_name or 'unknown',
                'timestamp': datetime.now().isoformat(),
                'tokens_used': len(content.split()),
                'query_type': 'summary',
                'raw_data': summary_data
            }

            await self.save_message(session_id, 'assistant', content, metadata)

            return ChatResponse(
                content=content,
                metadata=metadata,
                cached=False,
                tokens_used=metadata['tokens_used']
            )

        except Exception as e:
            error_message = f"Erro ao gerar resumo: {str(e)}"
            await self.save_message(session_id, 'assistant', error_message, {'error': True})
            return ChatResponse(content=error_message, metadata={'error': True}, cached=False)

    async def _handle_specific_search(self, session_id: str, query: str, context: Optional[Dict[str, Any]]) -> ChatResponse:
        """Handle specific document searches using RAG when available."""
        document_context = DocumentContext(documents=[], summaries=[], insights=[])

        try:
            # Tentar usar RAG se dispon√≠vel para busca sem√¢ntica
            try:
                from backend.services.rag_service import RAGService
                rag_service = RAGService()

                # Gerar embedding da query
                query_embedding = rag_service.embedding_service.generate_query_embedding(query)

                # Buscar documentos relevantes usando RAG
                similar_docs = rag_service.vector_store.get_document_context(
                    query_embedding=query_embedding,
                    max_documents=context.get('limit', 5) if context else 5,
                    max_chunks_per_document=2
                )

                if similar_docs:
                    # Extrair IDs dos documentos
                    doc_ids = [doc['fiscal_document_id'] for doc in similar_docs]

                    # Buscar documentos completos usando PostgreSQL direto
                    from backend.database.postgresql_storage import PostgreSQLStorage
                    db = PostgreSQLStorage()

                    documents = []
                    for doc_id in doc_ids:
                        doc = db.get_fiscal_document(doc_id)
                        if doc:
                            # Adicionar similaridade do RAG
                            doc_similarity = next((d['total_similarity'] for d in similar_docs if d['fiscal_document_id'] == doc_id), 0)
                            doc['rag_similarity'] = doc_similarity
                            documents.append(doc)

                    if documents:
                        document_context = DocumentContext(
                            documents=documents,
                            summaries=[],
                            insights=[]
                        )
                        logger.info(f"RAG search found {len(documents)} relevant documents")

            except Exception as rag_error:
                logger.warning(f"RAG search failed, using fallback: {rag_error}")
                # Fallback para busca por texto
                if context and 'document_types' in context:
                    relevant_docs = await self._search_documents(query, limit=context.get('limit', 5))
                    if relevant_docs:
                        documents = []
                        for doc in relevant_docs:
                            if isinstance(doc, tuple) and len(doc) > 0:
                                doc = doc[0]
                            documents.append(doc)

                        document_context = DocumentContext(
                            documents=documents,
                            summaries=[],
                            insights=[]
                        )

        except Exception as e:
            logger.error(f"Erro na busca espec√≠fica: {e}")

        # Get conversation history for context
        history_context = await self.get_conversation_context(session_id)

        # Prepare context for LLM
        context_prompt = self._prepare_context_prompt(query, document_context, context, history_context)

        # Generate response using the chat model
        messages = [
            SystemMessage(content=self.system_prompt),
            HumanMessage(content=f"{context_prompt}\n\nPergunta: {query}")
        ]

        try:
            response = self.model.invoke(messages, config={
                'temperature': 0.3,
                'max_tokens': 1000,
                'top_p': 0.9,
                'frequency_penalty': 0.5,
                'presence_penalty': 0.5
            })

            content = response.content if hasattr(response, 'content') else str(response)
            content = self._clean_response_content(content)

            # Create metadata
            metadata = {
                'model': self.model_name or 'unknown',
                'timestamp': datetime.now().isoformat(),
                'tokens_used': len(content.split()),
                'rag_used': len(document_context.documents) > 0,
                'documents_found': len(document_context.documents)
            }

            # Cache the response
            if context:
                await self.cache.cache_response(
                    query=query,
                    context=context,
                    response=content,
                    metadata=metadata,
                    query_type=context.get('query_type', 'general')
                )

            # Save to conversation history
            await self.save_message(session_id, 'assistant', content, metadata)

            return ChatResponse(
                content=content,
                metadata=metadata,
                cached=False,
                tokens_used=metadata['tokens_used']
            )

        except Exception as e:
            error_message = f"Desculpe, ocorreu um erro ao processar sua pergunta: {str(e)}"
            await self.save_message(session_id, 'assistant', error_message, {'error': True})
            return ChatResponse(
                content=error_message,
                metadata={'error': True, 'error_message': str(e)},
                cached=False
            )

    def _format_as_table(self, content: str) -> str:
        """Format lists and document information as Markdown tables."""
        try:
            # Check for document types
            doc_keywords = ['nfe', 'nf-e', 'cte', 'ct-e', 'mdfe', 'md-fe', 'nfse', 'nfs-e', 'nfce', 'nfc-e']
            
            # Check for bullet point lists
            lines = content.split('\n')
            bullet_points = [line.strip('-*‚Ä¢ ') for line in lines if line.strip().startswith(('- ', '* ', '‚Ä¢ '))]
            
            # If we found a list with more than 2 items, format as table
            if len(bullet_points) > 2:
                # Try to split each bullet into columns
                table_rows = []
                for point in bullet_points:
                    # Try to split on common separators
                    parts = re.split(r'[:\-]\s*', point, maxsplit=1)
                    if len(parts) == 2:
                        table_rows.append([parts[0].strip(), parts[1].strip()])
                    else:
                        table_rows.append([point.strip(), ''])
                
                # If we have at least 2 columns, format as table
                if table_rows and len(table_rows[0]) > 1:
                    table = ['| ' + ' | '.join(['Item', 'Descri√ß√£o']) + ' |',
                             '|' + '|'.join(['---'] * len(table_rows[0])) + '|']
                    for row in table_rows:
                        table.append('| ' + ' | '.join(row) + ' |')
                    return '\n'.join(table)
            
            # Document types table (special case)
            if any(keyword in content.lower() for keyword in doc_keywords):
                return """
| Documento | Nome Completo | Finalidade Principal |
|-----------|---------------|----------------------|
| **NF-e** | Nota Fiscal Eletr√¥nica | Documenta√ß√£o de opera√ß√µes com mercadorias |
| **NFC-e** | Nota Fiscal de Consumidor Eletr√¥nica | Vendas a consumidores finais |
| **CT-e** | Conhecimento de Transporte Eletr√¥nico | Documenta√ß√£o de servi√ßos de transporte |
| **MDF-e** | Manifesto de Documentos Fiscais | Agrupamento de documentos de transporte |
| **NFSe** | Nota Fiscal de Servi√ßo Eletr√¥nica | Documenta√ß√£o de presta√ß√£o de servi√ßos |
| **CF-e** | Cupom Fiscal Eletr√¥nico | Documenta√ß√£o de vendas no varejo (alguns estados) |

**Legenda**:
- **NF-e/NFC-e**: Documentos de venda
- **CT-e/MDF-e**: Documentos de transporte
- **NFSe/CF-e**: Documentos de servi√ßo/varejo
"""
            
            # If no special formatting applied, return original content
            return content
            
        except Exception as e:
            logger.warning(f"Error formatting as table: {e}")
            return content

    def _clean_response_content(self, content: str) -> str:
        """Clean up response content to remove repetitions and improve quality."""
        # First, try to format as table if appropriate
        if any(keyword in content.lower() for keyword in ['tabela', 'lista de', 'documentos fiscais', 'exemplo:', 'tipos de']):
            formatted = self._format_as_table(content)
            if formatted != content:  # Only return if formatting was applied
                return formatted
            
        # Split into sentences for better processing
        import re
        sentences = re.split(r'(?<=[.!?])\s+', content)
        
        # Remove duplicate sentences while preserving order
        seen = set()
        clean_sentences = []
        for sentence in sentences:
            # Normalize and check for duplicates
            norm_sent = ' '.join(sentence.lower().split())
            if norm_sent not in seen and len(norm_sent) > 10:  # Ignore very short sentences
                seen.add(norm_sent)
                clean_sentences.append(sentence)
        
        # Join back with proper spacing
        cleaned_content = ' '.join(clean_sentences)
        
        # Remove any remaining repeated phrases (simple heuristic)
        words = cleaned_content.split()
        unique_words = []
        for word in words:
            if len(unique_words) < 2 or word.lower() not in [w.lower() for w in unique_words[-3:]]:
                unique_words.append(word)
        
        return ' '.join(unique_words)

    def _prepare_context_prompt(
            self,
            query: str,
            document_context: DocumentContext,
            context: Optional[Dict[str, Any]],
            history_context: str = ""
    ) -> str:
        """Prepara o contexto para o prompt do LLM.
        
        Args:
            query: A consulta do usu√°rio
            document_context: Contexto dos documentos relevantes
            context: Contexto adicional (filtros, prefer√™ncias, etc.)
            history_context: Hist√≥rico da conversa
            
        Returns:
            str: Contexto formatado para o prompt
        """
        """Prepare context information for the LLM prompt."""
        context_parts = []
        has_documents = bool(document_context and document_context.documents)

        # Add document context if available
        if has_documents:
            context_parts.append("üìÑ Documentos dispon√≠veis para an√°lise:")
            
            # Tenta formatar como tabela se for uma lista de documentos
            if len(document_context.documents) > 1:
                # Extrai os campos mais importantes para a tabela
                table_header = "| Tipo | N√∫mero | Emissor | Data | Valor |\n"
                table_header += "|------|--------|---------|------|-------|\n"
                
                table_rows = []
                for doc in document_context.documents[:5]:  # Limita a 5 documentos na tabela
                    doc_type = doc.get('document_type', 'N/A')
                    doc_number = doc.get('document_number', 'N/A')
                    issuer = doc.get('issuer_name') or doc.get('issuer_cnpj', 'N/A')
                    date = doc.get('emission_date') or doc.get('created_at', 'N/A')
                    
                    # Tenta extrair o valor total dos dados extra√≠dos
                    total_value = 'N/A'
                    if 'extracted_data' in doc and doc['extracted_data']:
                        try:
                            if isinstance(doc['extracted_data'], str):
                                data = json.loads(doc['extracted_data'])
                            else:
                                data = doc['extracted_data']
                                
                            if isinstance(data, dict):
                                total_value = data.get('total', data.get('valor_total', 'N/A'))
                        except (json.JSONDecodeError, AttributeError):
                            pass
                    
                    table_rows.append(f"| {doc_type} | {doc_number} | {issuer} | {date} | {total_value} |")
                
                if table_rows:
                    context_parts.append(table_header + "\n".join(table_rows))
            
            # Se n√£o for uma tabela ou al√©m da tabela, mostra detalhes adicionais
            if len(document_context.documents) == 1 or len(document_context.documents) > 5:
                for doc in document_context.documents[:3]:  # Limita a 3 documentos detalhados
                    doc_info = [
                        f"- {k}: {v}" for k, v in doc.items()
                        if k not in ['content', 'embedding', 'extracted_data'] and v is not None
                    ]
                    if doc_info:
                        context_parts.append("\n".join(doc_info))
                        
                    # Adiciona dados extra√≠dos formatados, se dispon√≠veis
                    if 'extracted_data' in doc and doc['extracted_data']:
                        try:
                            if isinstance(doc['extracted_data'], str):
                                data = json.loads(doc['extracted_data'])
                            else:
                                data = doc['extracted_data']
                                
                            if isinstance(data, dict):
                                extracted_info = [
                                    f"- {k}: {v}" for k, v in data.items()
                                    if v is not None and k not in ['content', 'embedding']
                                ]
                                if extracted_info:
                                    context_parts.append("\nDados extra√≠dos:" + "\n" + "\n".join(extracted_info[:5]))  # Limita a 5 itens
                        except (json.JSONDecodeError, AttributeError):
                            pass
        
        # Add conversation history if available
        if history_context:
            context_parts.append(f"\nüí¨ Hist√≥rico da Conversa:\n{history_context}")

        # Add instructions for response
        if has_documents:
            context_parts.append(
                "\nüìù Instru√ß√µes para a resposta:\n"
                "1. Analise a pergunta e verifique se ela se refere aos documentos carregados.\n"
                "2. Se a pergunta for sobre os documentos, responda com base neles.\n"
                "3. Se a pergunta for mais geral ou n√£o houver documentos relevantes, use seu conhecimento geral.\n"
                "4. Estruture a resposta de forma clara e objetiva."
            )
        else:
            context_parts.append(
                "\n‚ÑπÔ∏è N√£o h√° documentos espec√≠ficos carregados. "
                "Voc√™ deve responder com base no seu conhecimento geral sobre documentos fiscais brasileiros.\n"
                "\nüìù Instru√ß√µes para a resposta:\n"
                "1. Responda de forma clara e completa, mesmo sem documentos espec√≠ficos.\n"
                "2. Use seu conhecimento sobre legisla√ß√£o fiscal brasileira.\n"
                "3. Se a pergunta for muito espec√≠fica e exigir documentos, explique isso ao usu√°rio.\n"
                "4. Formate a resposta de forma organizada e f√°cil de entender."
            )

        # Add insights if available (after instructions to keep them prominent)
        if has_documents and hasattr(document_context, 'insights') and document_context.insights:
            context_parts.append("\nüîç Insights identificados nos documentos:")
            for insight in document_context.insights[:3]:  # Limit to top 3 insights
                if isinstance(insight, dict):
                    if 'insight_type' in insight and 'insight_text' in insight:
                        context_parts.append(f"üìå {insight['insight_type'].title()}:")
                        context_parts.append(f"   {insight['insight_text']}")
                    elif 'insight_text' in insight:
                        context_parts.append(f"- {insight['insight_text']}")
                elif isinstance(insight, str):
                    context_parts.append(f"- {insight}")

        return "\n".join(context_parts) if context_parts else "Nenhum contexto espec√≠fico dispon√≠vel."

    def _clean_response_content(self, content: str) -> str:
        """Clean and format the LLM response content."""
        if not content:
            return ""

        # Remove extra whitespace and normalize line breaks
        cleaned = content.strip()

        # Fix common formatting issues
        cleaned = cleaned.replace('```json', '```')
        cleaned = cleaned.replace('```python', '```')

        return cleaned
